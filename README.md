# FastOnnxLoader ðŸš€ðŸ‘©â€ðŸš€
Loads in onnx files with less RAM. (Not necessarily faster) Usefuk for loading very large ONNX files onto the GPU with limited RAM.

This script is a test to load an ONNX file in OnnxRuntime with less RAM by first loading in an ONNX with no embedded weights, then sequentially loading in the weight files one by one.

Workflow
===
1. export the torch model using `torch.onnx.export()` which gives you your basic model.onnx file
2. If the file does not have separate weights, then use `python separate.py` on model.onnx
3. Use `python clear.py` on this new model.onnx file
4. Use the C# code to load in the `model_without_weights.onnx` and sequentially load in the weight files.

separate.py
===
This takes an onnx file and separates it into separate weight files. It makes sure to rename weights with illegal characters such as ":"

clear.py
===
If you already have an onnx file with separated weights (for example it might be a >2GB onnx). Then you can just use clear.py to create a **model_without_weights.onnx** file.


Issues
===
No issues at the moment


See also
===
https://github.com/microsoft/onnxruntime/issues/15429


![image](https://user-images.githubusercontent.com/33497043/230762304-1123df5c-e374-4614-8a5e-8ddc28452def.png)

